{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKqJCbj3pgeV95+6RMoZak",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshayabalaji23/-Dynamic-Data-Ingestion-and-Storage-in-HDFS-with-Automated-Hive-Integration/blob/main/%20Dynamic%20Data%20Ingestion%20and%20Storage%20in%20HDFS%20with%20Automated%20Hive%20Integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5xejKcbezHl"
      },
      "outputs": [],
      "source": [
        "# ===== User-configurable parameters =====\n",
        "import os\n",
        "\n",
        "# Pick a specific CSV from the Census site. Replace with the exact path you intend to use.\n",
        "DATASET_URL = \"https://www2.census.gov/programs-surveys/popest/datasets/2020/state/asrh/sc-est2020-alldata6.csv\"\n",
        "\n",
        "# Local filenames/paths\n",
        "LOCAL_FILE = \"census_data.csv\"  # saved name after download\n",
        "\n",
        "# HDFS locations\n",
        "HDFS_BASE_DIR = \"/user/$USER/census_data\"  # uses your unix user automatically\n",
        "HDFS_FILE_PATH = f\"{HDFS_BASE_DIR}/{LOCAL_FILE}\"\n",
        "\n",
        "# Hive settings\n",
        "HIVE_DB = \"census_db\"\n",
        "HIVE_TABLE = \"census_table\"\n",
        "\n",
        "# If your CSV delimiter is comma, keep as is. Otherwise change accordingly.\n",
        "CSV_DELIMITER = \",\"\n",
        "\n",
        "print(\"Configured:\\n\",\n",
        "      f\"DATASET_URL=\\t{DATASET_URL}\\n\",\n",
        "      f\"LOCAL_FILE=\\t{LOCAL_FILE}\\n\",\n",
        "      f\"HDFS_BASE_DIR=\\t{HDFS_BASE_DIR}\\n\",\n",
        "      f\"HDFS_FILE_PATH=\\t{HDFS_FILE_PATH}\\n\",\n",
        "      f\"HIVE_DB=\\t\\t{HIVE_DB}\\n\",\n",
        "      f\"HIVE_TABLE=\\t{HIVE_TABLE}\\n\",\n",
        "      f\"CSV_DELIMITER=\\t{CSV_DELIMITER}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "echo \"Checking accessibility (HEAD request)...\"\n",
        "curl -I -L \"$DATASET_URL\" | head -n 20 || {\n",
        "  echo \"\\n[ERROR] Could not access $DATASET_URL\" >&2\n",
        "  exit 1\n",
        "}"
      ],
      "metadata": {
        "id": "kmgb_qRHfE4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "echo \"Downloading dataset to $LOCAL_FILE ...\"\n",
        "wget -O \"$LOCAL_FILE\" \"$DATASET_URL\"\n",
        "echo \"Downloaded: $(ls -lh \"$LOCAL_FILE\")\""
      ],
      "metadata": {
        "id": "KD3tpx8GfLaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv, re\n",
        "\n",
        "def sniff_types(sample_rows, headers):\n",
        "    def is_int(x):\n",
        "        try:\n",
        "            int(x)\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "    def is_float(x):\n",
        "        try:\n",
        "            float(x)\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "    types = []\n",
        "    for col_idx, _ in enumerate(headers):\n",
        "        col_vals = [r[col_idx] for r in sample_rows if len(r) > col_idx and r[col_idx] != \"\"]\n",
        "        # prefer INT -> BIGINT, fallback to DOUBLE, else STRING\n",
        "        if col_vals and all(is_int(v) for v in col_vals):\n",
        "            types.append(\"BIGINT\")\n",
        "        elif col_vals and all(is_float(v) for v in col_vals):\n",
        "            types.append(\"DOUBLE\")\n",
        "        else:\n",
        "            types.append(\"STRING\")\n",
        "    return types\n",
        "\n",
        "with open(LOCAL_FILE, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)\n",
        "    # take first 100 rows as sample\n",
        "    sample = [next(reader) for _ in range(100)]\n",
        "\n",
        "clean_headers = []\n",
        "for h in header:\n",
        "    h2 = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", h.strip())\n",
        "    if re.match(r\"^[0-9]\", h2):\n",
        "        h2 = \"c_\" + h2\n",
        "    if not h2:\n",
        "        h2 = \"col\"\n",
        "    clean_headers.append(h2.lower())\n",
        "\n",
        "types = sniff_types(sample, header)\n",
        "schema_pairs = [f\"{c} {t}\" for c, t in zip(clean_headers, types)]\n",
        "ddl_cols = \",\\n  \".join(schema_pairs)\n",
        "\n",
        "create_stmt = f\"\"\"\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS {HIVE_DB}.{HIVE_TABLE} (\n",
        "  {ddl_cols}\n",
        ")\n",
        "ROW FORMAT DELIMITED\n",
        "FIELDS TERMINATED BY '{CSV_DELIMITER}'\n",
        "STORED AS TEXTFILE\n",
        "LOCATION '{HDFS_BASE_DIR}'\n",
        "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
        "\"\"\".strip()\n",
        "\n",
        "print(\"\\n---- Inferred Hive CREATE TABLE DDL ----\\n\")\n",
        "print(create_stmt)\n"
      ],
      "metadata": {
        "id": "fFF75cwvfTBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "echo \"Creating HDFS directory: $HDFS_BASE_DIR\"\n",
        "hadoop fs -mkdir -p \"$HDFS_BASE_DIR\"\n",
        "echo \"Uploading file to HDFS: $HDFS_FILE_PATH\"\n",
        "hadoop fs -put -f \"$LOCAL_FILE\" \"$HDFS_FILE_PATH\"\n",
        "echo \"Listing HDFS directory:\"\n",
        "hadoop fs -ls -h \"$HDFS_BASE_DIR\""
      ],
      "metadata": {
        "id": "oy_cNCAIfZYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$HIVE_DB\" \"$HIVE_TABLE\"\n",
        "set -euo pipefail\n",
        "HIVE_DB=\"$1\"; HIVE_TABLE=\"$2\";\n",
        "echo \"Creating Hive database if not exists: $HIVE_DB\"\n",
        "hive -e \"CREATE DATABASE IF NOT EXISTS ${HIVE_DB};\"\n",
        "echo \"Dropping existing table (if any): ${HIVE_DB}.${HIVE_TABLE}\"\n",
        "hive -e \"DROP TABLE IF EXISTS ${HIVE_DB}.${HIVE_TABLE};\""
      ],
      "metadata": {
        "id": "V5wAax0VfjfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$HDFS_BASE_DIR\" \"$CSV_DELIMITER\" \"$HIVE_DB\" \"$HIVE_TABLE\"\n",
        "set -euo pipefail\n",
        "HDFS_BASE_DIR=\"$1\"; CSV_DELIM=\"$2\"; HIVE_DB=\"$3\"; HIVE_TABLE=\"$4\";\n",
        "\n",
        "python - <<'PY'\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Load the previously generated DDL from Python memory by recomputing here (safe & deterministic)\n",
        "import csv, re\n",
        "LOCAL_FILE = os.environ.get('LOCAL_FILE', 'census_data.csv')\n",
        "HIVE_DB = os.environ.get('HIVE_DB', 'census_db')\n",
        "HIVE_TABLE = os.environ.get('HIVE_TABLE', 'census_table')\n",
        "CSV_DELIMITER = os.environ.get('CSV_DELIMITER', ',')\n",
        "HDFS_BASE_DIR = os.environ.get('HDFS_BASE_DIR', '/user/$USER/census_data')\n",
        "\n",
        "def sniff_types(sample_rows, headers):\n",
        "    def is_int(x):\n",
        "        try:\n",
        "            int(x); return True\n",
        "        except: return False\n",
        "    def is_float(x):\n",
        "        try:\n",
        "            float(x); return True\n",
        "        except: return False\n",
        "    types = []\n",
        "    for col_idx, _ in enumerate(headers):\n",
        "        col_vals = [r[col_idx] for r in sample_rows if len(r) > col_idx and r[col_idx] != \"\"]\n",
        "        if col_vals and all(is_int(v) for v in col_vals):\n",
        "            types.append(\"BIGINT\")\n",
        "        elif col_vals and all(is_float(v) for v in col_vals):\n",
        "            types.append(\"DOUBLE\")\n",
        "        else:\n",
        "            types.append(\"STRING\")\n",
        "    return types\n",
        "\n",
        "with open(LOCAL_FILE, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)\n",
        "    sample = []\n",
        "    for i, row in enumerate(reader):\n",
        "        if i >= 100: break\n",
        "        sample.append(row)\n",
        "\n",
        "clean_headers = []\n",
        "for h in header:\n",
        "    h2 = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", h.strip())\n",
        "    if re.match(r\"^[0-9]\", h2): h2 = \"c_\" + h2\n",
        "    if not h2: h2 = \"col\"\n",
        "    clean_headers.append(h2.lower())\n",
        "\n",
        "types = sniff_types(sample, header)\n",
        "schema_pairs = [f\"{c} {t}\" for c, t in zip(clean_headers, types)]\n",
        "ddl_cols = \",\\n  \".join(schema_pairs)\n",
        "create_stmt = f\"\"\"\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS {HIVE_DB}.{HIVE_TABLE} (\n",
        "  {ddl_cols}\n",
        ")\n",
        "ROW FORMAT DELIMITED\n",
        "FIELDS TERMINATED BY '{CSV_DELIMITER}'\n",
        "STORED AS TEXTFILE\n",
        "LOCATION '{HDFS_BASE_DIR}'\n",
        "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
        "\"\"\".strip()\n",
        "\n",
        "# Save DDL to a temp SQL file for execution\n",
        "Path(\"create_table.sql\").write_text(create_stmt)\n",
        "print(\"Saved Hive DDL to create_table.sql:\\n\", create_stmt)\n",
        "PY\n",
        "\n",
        "echo \"Creating external table via Hive...\"\n",
        "hive -f create_table.sql\n",
        "echo \"Describe table:\"\n",
        "hive -e \"USE ${HIVE_DB}; DESCRIBE EXTENDED ${HIVE_TABLE};\""
      ],
      "metadata": {
        "id": "XQXFgCdqfnGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$HIVE_DB\" \"$HIVE_TABLE\"\n",
        "set -euo pipefail\n",
        "HIVE_DB=\"$1\"; HIVE_TABLE=\"$2\";\n",
        "echo \"Row count:\"\n",
        "hive -e \"SELECT COUNT(*) AS row_count FROM ${HIVE_DB}.${HIVE_TABLE};\"\n",
        "\n",
        "echo \"\\nPreview 10 rows:\"\n",
        "hive -e \"SELECT * FROM ${HIVE_DB}.${HIVE_TABLE} LIMIT 10;\""
      ],
      "metadata": {
        "id": "XmPf9ItRfznV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$HIVE_DB\" \"$HIVE_TABLE\"\n",
        "set -euo pipefail\n",
        "HIVE_DB=\"$1\"; HIVE_TABLE=\"$2\";\n",
        "\n",
        "echo \"Top 10 names by total population estimate (if columns exist):\"\n",
        "hive -e \"\n",
        "USE ${HIVE_DB};\n",
        "SET hive.mapred.mode=nonstrict;\n",
        "SELECT name, SUM(COALESCE(popeSTIMATE2020, 0)) AS total_est_2020\n",
        "FROM ${HIVE_TABLE}\n",
        "GROUP BY name\n",
        "ORDER BY total_est_2020 DESC\n",
        "LIMIT 10;\" || true\n",
        "\n",
        "echo \"\\nAge distribution (if 'age' column exists):\"\n",
        "hive -e \"\n",
        "USE ${HIVE_DB};\n",
        "SELECT age, COUNT(1) AS rows_per_age\n",
        "FROM ${HIVE_TABLE}\n",
        "GROUP BY age\n",
        "ORDER BY age\n",
        "LIMIT 20;\" || true"
      ],
      "metadata": {
        "id": "W0TufdTBgSPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "cat > refresh_census.sh <<'EOF'\n",
        "#!/usr/bin/env bash\n",
        "set -euo pipefail\n",
        "\n",
        "DATASET_URL=\"${DATASET_URL}\"\n",
        "LOCAL_FILE=\"${LOCAL_FILE}\"\n",
        "HDFS_BASE_DIR=\"${HDFS_BASE_DIR}\"\n",
        "HIVE_DB=\"${HIVE_DB}\"\n",
        "HIVE_TABLE=\"${HIVE_TABLE}\"\n",
        "CSV_DELIMITER=\"${CSV_DELIMITER}\"\n",
        "\n",
        "echo \"[1/5] Downloading $DATASET_URL ...\"\n",
        "wget -O \"$LOCAL_FILE\" \"$DATASET_URL\"\n",
        "\n",
        "echo \"[2/5] Creating HDFS dir $HDFS_BASE_DIR ...\"\n",
        "hadoop fs -mkdir -p \"$HDFS_BASE_DIR\"\n",
        "\n",
        "echo \"[3/5] Uploading to HDFS ...\"\n",
        "hadoop fs -put -f \"$LOCAL_FILE\" \"$HDFS_BASE_DIR/$LOCAL_FILE\"\n",
        "\n",
        "echo \"[4/5] Building Hive DDL from CSV header ...\"\n",
        "python - <<'PY'\n",
        "import os, csv, re\n",
        "LOCAL_FILE = os.environ.get('LOCAL_FILE', 'census_data.csv')\n",
        "HIVE_DB = os.environ.get('HIVE_DB', 'census_db')\n",
        "HIVE_TABLE = os.environ.get('HIVE_TABLE', 'census_table')\n",
        "CSV_DELIMITER = os.environ.get('CSV_DELIMITER', ',')\n",
        "HDFS_BASE_DIR = os.environ.get('HDFS_BASE_DIR', '/user/$USER/census_data')\n",
        "def sniff_types(sample_rows, headers):\n",
        "    def is_int(x):\n",
        "        try: int(x); return True\n",
        "        except: return False\n",
        "    def is_float(x):\n",
        "        try: float(x); return True\n",
        "        except: return False\n",
        "    types = []\n",
        "    for col_idx, _ in enumerate(headers):\n",
        "        col_vals = [r[col_idx] for r in sample_rows if len(r) > col_idx and r[col_idx] != \"\"]\n",
        "        if col_vals and all(is_int(v) for v in col_vals): types.append('BIGINT')\n",
        "        elif col_vals and all(is_float(v) for v in col_vals): types.append('DOUBLE')\n",
        "        else: types.append('STRING')\n",
        "    return types\n",
        "with open(LOCAL_FILE, newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    header = next(reader)\n",
        "    sample = [next(reader) for _ in range(100)]\n",
        "clean_headers = []\n",
        "for h in header:\n",
        "    h2 = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", h.strip())\n",
        "    if re.match(r\"^[0-9]\", h2): h2 = \"c_\" + h2\n",
        "    if not h2: h2 = \"col\"\n",
        "    clean_headers.append(h2.lower())\n",
        "types = sniff_types(sample, header)\n",
        "ddl_cols = \",\\n  \".join([f\"{c} {t}\" for c,t in zip(clean_headers, types)])\n",
        "stmt = f\"\"\"\n",
        "CREATE DATABASE IF NOT EXISTS {HIVE_DB};\n",
        "DROP TABLE IF EXISTS {HIVE_DB}.{HIVE_TABLE};\n",
        "CREATE EXTERNAL TABLE IF NOT EXISTS {HIVE_DB}.{HIVE_TABLE} (\n",
        "  {ddl_cols}\n",
        ")\n",
        "ROW FORMAT DELIMITED\n",
        "FIELDS TERMINATED BY '{CSV_DELIMITER}'\n",
        "STORED AS TEXTFILE\n",
        "LOCATION '{HDFS_BASE_DIR}'\n",
        "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
        "\"\"\"\n",
        "open('refresh_create.sql','w').write(stmt)\n",
        "print(stmt)\n",
        "PY\n",
        "\n",
        "echo \"[5/5] Applying Hive DDL ...\"\n",
        "hive -f refresh_create.sql\n",
        "echo \"Done.\"\n",
        "EOF\n",
        "\n",
        "chmod +x refresh_census.sh\n",
        "echo \"Created refresh_census.sh\"\n",
        "ls -lh refresh_census.sh"
      ],
      "metadata": {
        "id": "rVDz14T2gdik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash -s \"$HIVE_DB\" \"$HIVE_TABLE\"\n",
        "set -euo pipefail\n",
        "HIVE_DB=\"$1\"; HIVE_TABLE=\"$2\";\n",
        "echo \"Databases:\"; hive -e \"SHOW DATABASES;\" | sed -n '1,50p'\n",
        "echo \"\\nTables in ${HIVE_DB}:\"; hive -e \"USE ${HIVE_DB}; SHOW TABLES;\"\n",
        "echo \"\\nSample data:\"; hive -e \"SELECT * FROM ${HIVE_DB}.${HIVE_TABLE} LIMIT 5;\""
      ],
      "metadata": {
        "id": "l_YCx7IrgjIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nkEaEHlbjoUF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}